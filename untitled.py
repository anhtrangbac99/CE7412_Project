# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xeplEO4Y8zhIPFDuxcEKKzVeS31SrLmG
"""

# from google.colab import drive
# drive.mount('/content/drive')

# cd /content/drive/MyDrive/CE7412-Project-main

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import pandas as pd

import os
import tqdm

batch_size = 4
device = 'cuda:0'

"""# Conv + LSTM """

class ConvLSTM(nn.Module):
    def __init__(self, n_features, n_hidden, seq_len, n_layers):
        super(ConvLSTM, self).__init__()
        self.n_hidden = n_hidden
        self.seq_len = seq_len
        self.n_layers = n_layers
        self.c1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size = 2, stride = 1) # Add a 1D CNN layer
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=n_hidden,
            num_layers=n_layers
        )
        self.linear = nn.Linear(in_features=n_hidden, out_features=1)
    def reset_hidden_state(self):
        self.hidden = (
            torch.zeros(self.n_layers, self.seq_len-1, self.n_hidden),
            torch.zeros(self.n_layers, self.seq_len-1, self.n_hidden)
        )
    def forward(self, sequences):
        sequences = self.c1(sequences.view(len(sequences), 1, -1))
        self.hidden = self.reset_hidden_state()
        lstm_out, self.hidden = self.lstm(
            sequences.view(len(sequences), self.seq_len-1, -1),
            self.hidden
        )
        last_time_step = lstm_out.view(self.seq_len-1, len(sequences), self.n_hidden)[-1]
        y_pred = self.linear(last_time_step)
        return y_pred

"""# Read Data"""

import pickle
import numpy as np

#train data
virus_data = []
for idx, file in enumerate(os.listdir('data/training/')):
    array = pd.read_pickle(os.path.join('data/training',file))
    for i in array:
        virus_data.append(np.array([idx,np.array(i)]))
    del array

#val data

# virus_data_val = []
# for idx, file in enumerate(os.listdir('data/validation/')):
#     array = pd.read_pickle(os.path.join('data/validation',file))
#     for i in array:
#         virus_data_val.append([idx,i])

# encodings_ = []
# labels_ = []
# for label, fasta_filename in enumerate(os.listdir('data/training/')):
#     with open(os.path.join('data/training',fasta_filename), 'rb') as pkfile:
#         file_encodings = pickle.load(pkfile)
#         encodings_.extend([np.array(enc) for enc in file_encodings])  # Convert nested lists to NumPy arrays
#         labels_.extend([label] * len(file_encodings))
#         del file_encodings

# encodings = np.array(encodings_, dtype=object)
# labels = np.array(labels_)
# del encodings_
# del labels_
# # Find the maximum sequence length
# max_seq_len = max([enc.shape[0] for enc in encodings])

# # Pad encodings with zeros to create tensors
# encodings_padded = []
# for enc in encodings:
#     pad_rows = max_seq_len - enc.shape[0]
#     enc_padded = np.pad(enc, (0, pad_rows), mode='constant', constant_values=0)
#     encodings_padded.append(enc_padded)
# del encodings
# encodings = np.stack(encodings_padded)
# del encodings_padded

virus_data = np.array(virus_data)

max_seq_len = max([enc.shape[0] for enc in virus_data[...,1]])

# Pad encodings with zeros to create tensors
encodings_padded = []
for enc in  virus_data[...,1]:
    pad_rows = max_seq_len - enc.shape[0]
    enc_padded = np.pad(enc, (0, pad_rows), mode='constant', constant_values=0)
    encodings_padded.append(enc_padded)

# mu = sum(arr) / len(arr)
# tmp = arr - mu
# tmp = tmp * tmp
# sd = np.sum(tmp) / len(arr)
def normalize_encodings(encodings):
    mean = np.mean(encodings)
    std = np.std(encodings)
    return (encodings - mean) / std

encodings_padded = normalize_encodings(encodings_padded)
"""# Dataset"""

class VirusDataset(Dataset):
    def __init__(self,data,label):
        self.data = data
        self.label = label
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self,idx):
        return torch.Tensor(self.data[idx]),self.label[idx]

train_dataset = VirusDataset(encodings_padded,virus_data[...,0])
# val_dataset = VirusDataset(virus_data_val)

train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True)

"""# Training"""

model = ConvLSTM(n_features = 1 , n_hidden = 64, seq_len = max_seq_len, n_layers = 2)

model.to(device)

learning_rate = 0.00001
num_epochs = 20
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

def train_epoch(model, dataloader, criterion, optimizer):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    for inputs, targets in tqdm.tqdm(dataloader):
        # Move tensors to the correct device
        inputs = inputs.to(device)
        targets = targets.type(torch.FloatTensor).to(device)

        # Forward pass
        outputs = model(inputs)
        
        # print(outputs.squeeze().shape)
        # print(targets.shape)
        loss = criterion(outputs.squeeze(), targets)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate the number of correct predictions
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == targets).sum().item()
        total_predictions += targets.size(0)

    average_loss = running_loss / len(dataloader)
    accuracy = correct_predictions / total_predictions

    return average_loss, accuracy

for epoch in range(num_epochs):
    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer)
    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}")


